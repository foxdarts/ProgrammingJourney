{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook is for unit 4 for CS379 for David Finger. teaching deep learning for a neural network of alice in wonderland txt. started 10/8/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in txt:  148574\n",
      "Total unique characters in TXT:  46\n"
     ]
    }
   ],
   "source": [
    "import numpy #used to reshape txt file for use within keras\n",
    "import sys #used to output code at the end\n",
    "\n",
    "from keras.models import Sequential #this is imported for the shape of the model\n",
    "from keras.layers import Dense #this is used to show densities within keras output\n",
    "from keras.layers import Dropout #this is used to help with missed values\n",
    "from keras.layers import LSTM #this is used to inport everything into a LSTM model\n",
    "from keras.callbacks import ModelCheckpoint #\n",
    "from keras.utils import np_utils #allows the use of numpty utilities within keras\n",
    "\n",
    "\n",
    "filename = \"alice_in_wonderland.txt\" #this loads the txt file to be used\n",
    "raw_text = open(filename, 'r', encoding = 'utf-8').read()\n",
    "raw_text = raw_text.lower() #this makes everything lowercase within the txt file\n",
    "\n",
    "Chars = sorted(list(set(raw_text)))\n",
    "Chartoint = dict((c, i) for i, c in enumerate(Chars)) #creates map for character to intigers\n",
    "inttoChar = dict((i, c) for i, c in enumerate(Chars))\n",
    "\n",
    "NChar = len(raw_text) #gives us a number for characters in txt\n",
    "NVocab = len(Chars)  #gives us a number for unique characters in txt\n",
    "print (\"Total characters in txt: \", NChar)\n",
    "print (\"Total unique characters in TXT: \", NVocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above code loads the entire txt file to be read, counts the number of characters within it and the number of unique characters and prints them both at the end.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total patterns:  148474\n"
     ]
    }
   ],
   "source": [
    "#the training for the book starts here using sequence lengths of 100 to create pairs of integers\n",
    "\n",
    "seq_length = 100\n",
    "dataX = [] #splits the dataset into training data sets\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, NChar - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length] #sets an instream to be between i - 100 in the character count for the book\n",
    "    seq_out = raw_text[i + seq_length] #sets the outstream to be i + the total sequence\n",
    "\n",
    "    dataX.append([Chartoint[Chars] for Chars in seq_in])\n",
    "    dataY.append(Chartoint[seq_out])\n",
    "\n",
    "\n",
    "NPat = len(dataX) #gathers number of paterns created\n",
    "print (\"total patterns: \", NPat) #prints number of trained patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above code section takes the txt data and breaks it into patterns of trained sets so that we know how many splits happen within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.reshape(dataX, (NPat, seq_length, 1)) #reshapes txt data to an array so we can use it with keras\n",
    "\n",
    "X = X / float(NVocab) #normalizes the dataset between characters and words\n",
    "\n",
    "Y = np_utils.to_categorical(dataY) #hot encodes the output into a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above code starts the transformation process for the text into an array for use within keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1160/1160 [==============================] - 1528s 1s/step - loss: 2.8513\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.85129, saving model to weights-improvement-01-2.8513-bigger.hdf5\n",
      "Epoch 2/5\n",
      "1160/1160 [==============================] - 1719s 1s/step - loss: 2.5296\n",
      "\n",
      "Epoch 00002: loss improved from 2.85129 to 2.52964, saving model to weights-improvement-02-2.5296-bigger.hdf5\n",
      "Epoch 3/5\n",
      "1160/1160 [==============================] - 1879s 2s/step - loss: 2.3543\n",
      "\n",
      "Epoch 00003: loss improved from 2.52964 to 2.35431, saving model to weights-improvement-03-2.3543-bigger.hdf5\n",
      "Epoch 4/5\n",
      "1160/1160 [==============================] - 2014s 2s/step - loss: 2.2106\n",
      "\n",
      "Epoch 00004: loss improved from 2.35431 to 2.21059, saving model to weights-improvement-04-2.2106-bigger.hdf5\n",
      "Epoch 5/5\n",
      "1160/1160 [==============================] - 1967s 2s/step - loss: 2.1049\n",
      "\n",
      "Epoch 00005: loss improved from 2.21059 to 2.10492, saving model to weights-improvement-05-2.1049-bigger.hdf5\n"
     ]
    }
   ],
   "source": [
    "Model = Sequential() #sets up the model for output\n",
    "Model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2]), return_sequences = True)) #clarofies the exact size\n",
    "Model.add(Dropout(0.2)) #sets dropout rate for characters in training sets\n",
    "Model.add(LSTM(256))\n",
    "Model.add(Dropout(0.2))\n",
    "Model.add(Dense(Y.shape[1], activation = 'softmax')) #sets a softmax cap for density\n",
    "Model.compile(loss = \"categorical_crossentropy\", optimizer = 'adam') #compiles the data into a set loss category\n",
    "\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint] #creates a weighted txt file for the text to compare too\n",
    "\n",
    "Model.fit(X, Y, epochs = 5, batch_size = 128, callbacks = callbacks_list) #sets the amount of tests to run to 5\n",
    "\n",
    "filename = \"weights-improvement-05-2.1131-bigger.hdf5\"\n",
    "Model.load_weights(filename)\n",
    "Model.compile(loss = \"categorical_crossentropy\", optimizer = 'adam') #compiles the data into a set loss category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above code sets the density, droprate, and neural network together for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: \n",
      "\"  know about this business?' the king said to\n",
      "alice.\n",
      "\n",
      "  `nothing,' said alice.\n",
      "\n",
      "  `nothing whatever?' \"\n",
      " said the ming, `nd she sabbit wou would the sueen sfar the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of the sueen of th\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "Start = numpy.random.randint(0, len(dataX) - 1) #selects a random passage from the txt.\n",
    "\n",
    "Pat = dataX[Start] #sets the seed to a variable\n",
    "\n",
    "print (\"Seed: \") #prints the random selection of text to Start\n",
    "print (\"\\\"\", ''.join([inttoChar[value] for value in Pat]), \"\\\"\") #prints the next section\n",
    "\n",
    "for i in range(500) :\n",
    "    X = numpy.reshape(Pat, (1, len(Pat), 1))\n",
    "    X = X / float(NVocab)\n",
    "    Predict = Model.predict(X, verbose = 0)\n",
    "    Index = numpy.argmax(Predict)\n",
    "    result = inttoChar[Index]\n",
    "    seq_in = [inttoChar[value] for value in Pat]\n",
    "    sys.stdout.write(result)\n",
    "    Pat.append(Index)\n",
    "    Pat = Pat[1 : len(Pat)]\n",
    "\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above code prints a randomly selected segment of the txt and is supposed to print the next 500 cahracters in order.  it doesnt seem to want to do that but will print what i think is a copy of the next character 500 times :)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "097ff149f4f1647f8cfefcf116c7b1f0ab6b5406ef392ab0b19075a3fb2e7c97"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
